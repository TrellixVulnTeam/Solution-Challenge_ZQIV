# GDSC Dalhousie University

## What the app is
Our solution translates American Sign Language to English text by collecting real-time images and matching it against a preset of images trained by us using machine-learning and TensorFlow.
The application is built using ReactJS for frontend and Python for the backend processing.
As seen on the screen, the app reads the gesture, identifies the pattern, and displays the translation with an accuracy of over 90% approximately.
We decided to use a website instead of an app as it was more feasible considering the resources that we had, and in terms of early adoption, people with communication disorders may not be willing to download an app, however using a website should be easier for them. If we have enough users for validation, we can shift our solution from a website to an app.
We did not focus initially on translating ASL to text, as
However, in the future we would like to incorporate ASL to text into our solution as well to have an all-in-one solution without asking people to switch apps/websites.

You can run it [here](https://zkaptan.github.io/Solution-Challenge/)!
